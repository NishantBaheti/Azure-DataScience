{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import style \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "from scipy import signal\n",
    "\n",
    "style.use(\"ggplot\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['square', 'circle', 'triangle']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = \"./data/shapes/\"\n",
    "\n",
    "shapes_classes = os.listdir(data_folder)\n",
    "shapes_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (128,128)\n",
    "batch_size = 30\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1.0/255,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 840 images belonging to 3 classes.\n",
      "Found 360 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "training_generator = datagen.flow_from_directory(\n",
    "    directory=data_folder,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "testing_generator = datagen.flow_from_directory(\n",
    "    directory=data_folder,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = training_generator.class_indices.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    MaxPooling2D,\n",
    "    Dropout,\n",
    "    Flatten\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution \n",
    "\n",
    "\n",
    "[Understanding Convolution](https://nishantbaheti.github.io/machineLearningExploration/MathExploration/Convolution.html#Understanding-Convolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input layer accepts an image and applies a convolution that uses 32 6x6 filters and a rectified linear unit activation function\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=32, \n",
    "        kernel_size=(6, 6), \n",
    "        input_shape=training_generator.image_shape, \n",
    "        activation='relu'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "\n",
    "- After extracting feature values from images, pooling (or downsampling) layers are used to reduce the number of feature values while retaining the key differentiating features that have been extracted.\n",
    "- The feature map extracted by a filter in a convolutional layer contains an array of feature values.\n",
    "- A pooling kernel is used to reduce the number of feature values. In this case, the kernel size is 2x2, so it will produce an array with quarter the number of feature values.\n",
    "- The pooling kernel is convolved across the feature map, retaining only the highest pixel value in each position.\n",
    "\n",
    "\n",
    "        0   0   0                             155 255\n",
    "        0  155 255  =>  apply 2X2 pooling =>  155 255\n",
    "        155 0   0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Next we'll add a max pooling layer with a 2x2 patch\n",
    "model.add(\n",
    "    MaxPooling2D(\n",
    "        pool_size=(2,2)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can add as many layers as we think necessary - here we'll add another convolution and max pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=32, \n",
    "        kernel_size=(6, 6), \n",
    "        activation='relu'\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    MaxPooling2D(\n",
    "        pool_size=(2, 2)\n",
    "    )\n",
    ")\n",
    "\n",
    "# And another set\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=32, \n",
    "        kernel_size=(6, 6), \n",
    "        activation='relu'\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    MaxPooling2D(\n",
    "        pool_size=(2, 2)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping layer\n",
    "\n",
    "One of the most difficult challenges in a CNN is the avoidance of overfitting, where the resulting model performs well with the training data but doesn't generalize well to new data on which it wasn't trained. One technique you can use to mitigate overfitting is to include layers in which the training process randomly eliminates (or \"drops\") feature maps. This may seem counterintuitive, but it's an effective way to ensure that the model doesn't learn to be over-dependent on the training images.\n",
    "\n",
    "Other techniques you can use to mitigate overfitting include randomly flipping, mirroring, or skewing the training images to generate data that varies between training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dropout layer randomly drops some nodes to reduce inter-dependencies (which can cause over-fitting)\n",
    "model.add(\n",
    "    Dropout(0.2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening layer\n",
    "\n",
    "After using convolutional and pooling layers to extract the salient features in the images, the resulting feature maps are multidimensional arrays of pixel values. A flattening layer is used to flatten the feature maps into a vector of values that can be used as input to a fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the feature maps \n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a fully-connected output layer with a predicted probability for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 123, 123, 32)      3488      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 61, 61, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 56, 56, 32)        36896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 23, 23, 32)        36896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 11, 11, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 11, 11, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3872)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 11619     \n",
      "=================================================================\n",
      "Total params: 88,899\n",
      "Trainable params: 88,899\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# (softmax ensures all probabilities sum to 1)\n",
    "model.add(\n",
    "    Dense(\n",
    "        training_generator.num_classes, \n",
    "        activation='softmax'\n",
    "    )\n",
    ")\n",
    "\n",
    "# With the layers defined, we can now compile the model for categorical (multi-class) classification\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "28/28 [==============================] - 53s 2s/step - loss: 0.7798 - accuracy: 0.6464 - val_loss: 0.4189 - val_accuracy: 0.8250\n",
      "Epoch 2/5\n",
      "28/28 [==============================] - 54s 2s/step - loss: 0.2636 - accuracy: 0.8905 - val_loss: 0.1260 - val_accuracy: 0.9778\n",
      "Epoch 3/5\n",
      "28/28 [==============================] - 50s 2s/step - loss: 0.0759 - accuracy: 0.9821 - val_loss: 0.0149 - val_accuracy: 0.9972\n",
      "Epoch 4/5\n",
      "28/28 [==============================] - 46s 2s/step - loss: 0.0199 - accuracy: 0.9964 - val_loss: 0.0262 - val_accuracy: 0.9889\n",
      "Epoch 5/5\n",
      "28/28 [==============================] - 45s 2s/step - loss: 0.0096 - accuracy: 0.9976 - val_loss: 0.0086 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Train the model over 5 epochs using 30-image batches and using the validation holdout dataset for validation\n",
    "num_epochs = 5\n",
    "history = model.fit(\n",
    "    training_generator,\n",
    "    steps_per_epoch = training_generator.samples // batch_size,\n",
    "    validation_data = testing_generator, \n",
    "    validation_steps = testing_generator.samples // batch_size,\n",
    "    epochs = num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.7798408269882202,\n",
       "  0.2636471390724182,\n",
       "  0.07594814151525497,\n",
       "  0.01989593915641308,\n",
       "  0.009630522690713406],\n",
       " 'accuracy': [0.6464285850524902,\n",
       "  0.8904761672019958,\n",
       "  0.9821428656578064,\n",
       "  0.9964285492897034,\n",
       "  0.9976190328598022],\n",
       " 'val_loss': [0.41894739866256714,\n",
       "  0.12595893442630768,\n",
       "  0.014909405261278152,\n",
       "  0.026197640225291252,\n",
       "  0.008617321960628033],\n",
       " 'val_accuracy': [0.824999988079071,\n",
       "  0.9777777791023254,\n",
       "  0.9972222447395325,\n",
       "  0.9888888597488403,\n",
       "  1.0]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python383jvsc74a57bd01da5964c5502736b4e0a0c4398fb3b913682175f516e99bd48540f11726a612c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
